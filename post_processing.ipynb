{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "from skimage import filters\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond = morphology.diamond(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir(\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/predictions_segformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds_standard = []\n",
    "preds_eroded = []\n",
    "preds_dilated = []\n",
    "preds_eroded_dilated = []\n",
    "preds_dilated_eroded = []\n",
    "\n",
    "for im in images:\n",
    "    image = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/predictions_segformer/{im}\")\n",
    "    image_binary = image > 0\n",
    "    gt = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/groundtruth/{im}\")\n",
    "    gt = np.round(gt/255)\n",
    "\n",
    "    eroded = morphology.binary_erosion(image_binary, diamond)\n",
    "    eroded_dilated = morphology.binary_dilation(eroded, diamond)\n",
    "    dilated = morphology.binary_dilation(image_binary, diamond)\n",
    "    dilated_eroded = morphology.binary_erosion(dilated, diamond)\n",
    "\n",
    "    labels.extend(gt.flatten())\n",
    "    preds_standard.extend(image_binary.flatten())\n",
    "    preds_eroded.extend(eroded.flatten())\n",
    "    preds_dilated.extend(dilated.flatten())\n",
    "    preds_eroded_dilated.extend(eroded_dilated.flatten())\n",
    "    preds_dilated_eroded.extend(dilated_eroded.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation(pred, gt):\n",
    "    accuracy = accuracy_score(gt, pred)\n",
    "    f1 = f1_score(gt, pred, pos_label=1)\n",
    "    recall = recall_score(gt, pred, pos_label=1)\n",
    "    precision = precision_score(gt, pred, pos_label=1)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    # return accuracy, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948439375\n",
      "F1: 0.8765916516078075\n",
      "Recall: 0.8642055602011005\n",
      "Precision: 0.8893379490580613\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_standard, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333403125\n",
      "F1: 0.8221232864718101\n",
      "Recall: 0.7269879614021582\n",
      "Precision: 0.9459068423335086\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_eroded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9348090625\n",
      "F1: 0.8585259397254625\n",
      "Recall: 0.9334905444726127\n",
      "Precision: 0.7947064850015819\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_dilated, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94856125\n",
      "F1: 0.8767557757159372\n",
      "Recall: 0.8634725964466743\n",
      "Precision: 0.8904540221406704\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_eroded_dilated, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9484371875\n",
      "F1: 0.8766459258639031\n",
      "Recall: 0.8646760137980058\n",
      "Precision: 0.888951894619378\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_dilated_eroded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import matplotlib.pyplot as plt\n",
    "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral, create_pairwise_gaussian\n",
    "# import cv2\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "# from osgeo import gdal\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "from PIL import Image\n",
    "# import evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tqdm.notebook import tqdm\n",
    "# import wandb\n",
    "\n",
    "from mask_to_submission import masks_to_submission\n",
    "\n",
    "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state dict loaded from ./models/finetuned_segformer_15_geocropdeg.pth\n"
     ]
    }
   ],
   "source": [
    "root_dir = './data'\n",
    "image_processor = SegformerImageProcessor(reduce_labels=False, do_resize=False)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\",\n",
    "                                                         num_labels=2,\n",
    ")\n",
    "\n",
    "# Load the model state dict\n",
    "model_path = \"./models/finetuned_segformer_15_geocropdeg.pth\" \n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(f\"Model state dict loaded from {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir(\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/predictions_segformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds_standard = []\n",
    "preds_CRF = []\n",
    "preds_eroded_dilated = []\n",
    "\n",
    "for im in sorted(images):\n",
    "    # Get image, segmentation and ground truth\n",
    "    image = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/images/{im}\")\n",
    "    segm = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/predictions_segformer/{im}\")\n",
    "    segm_binary = segm > 0\n",
    "    segm_binary = segm_binary.astype(float)\n",
    "    segm_rgb = gray2rgb(segm)\n",
    "    gt = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/groundtruth/{im}\")\n",
    "    gt = np.round(gt/255)\n",
    "\n",
    "    # Get the prediction probabilities\n",
    "\n",
    "    model.eval() \n",
    "    image = Image.open(f\"./data/validation/images/{im}\")\n",
    "    # Prepare the image for the model\n",
    "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    upsampled_logits = nn.functional.interpolate(logits, size=(400,400), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    p = torch.nn.functional.softmax(upsampled_logits, dim=1)\n",
    "\n",
    "    p = p[0].cpu().numpy()\n",
    "\n",
    "    # Use the CRF to get the final prediction\n",
    "    # Get the unary from the softmax output\n",
    "    W, H, NLABELS = gt.shape[0], gt.shape[1], 2\n",
    "    U = unary_from_softmax(p)\n",
    "\n",
    "    # Horizontal\n",
    "    d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "    d.setUnaryEnergy(U)\n",
    "    #d.addPairwiseGaussian(sxy=(1, 25), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    d.addPairwiseGaussian(sxy=(25, 1), compat=15, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    Q, tmp1, tmp2 = d.startInference()\n",
    "    for _ in range(50):\n",
    "        d.stepInference(Q, tmp1, tmp2)\n",
    "    kl1 = d.klDivergence(Q) / (H*W)\n",
    "    map_soln_h = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "    # Vertical\n",
    "    d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "    d.setUnaryEnergy(U)\n",
    "    d.addPairwiseGaussian(sxy=(1, 25), compat=15, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseGaussian(sxy=(25, 1), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    Q, tmp1, tmp2 = d.startInference()\n",
    "    for _ in range(50):\n",
    "        d.stepInference(Q, tmp1, tmp2)\n",
    "    kl2 = d.klDivergence(Q) / (H*W)\n",
    "    map_soln_v = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "    # Together\n",
    "    map_soln = (map_soln_h + map_soln_v) > 0\n",
    "    soln_array = np.array(map_soln)\n",
    "    soln_array = soln_array.astype(float)\n",
    "\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # plt.subplot(1,3,1); plt.imshow(segm_binary);\n",
    "    # plt.title('Original'); plt.axis('off');\n",
    "    # plt.subplot(1,3,2); plt.imshow(map_soln);\n",
    "    # plt.title('MAP Solution with DenseCRF'); plt.axis('off');\n",
    "    # plt.subplot(1,3,3); plt.imshow(gt);\n",
    "    # plt.title('Ground Truth'); plt.axis('off');\n",
    "    # plt.show()\n",
    "\n",
    "    eroded = morphology.binary_erosion(soln_array, diamond)\n",
    "    eroded_dilated = morphology.binary_dilation(eroded, diamond)\n",
    "\n",
    "    # Get the metrics\n",
    "    labels.extend(gt.flatten())\n",
    "    preds_standard.extend(segm_binary.flatten())\n",
    "    preds_CRF.extend(soln_array.flatten())\n",
    "    preds_eroded_dilated.extend(eroded_dilated.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948439375\n",
      "F1: 0.8765916516078075\n",
      "Recall: 0.8642055602011005\n",
      "Precision: 0.8893379490580613\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_standard, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9519078125\n",
      "F1: 0.8860549367802774\n",
      "Recall: 0.8824411674918039\n",
      "Precision: 0.8896984259625479\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_CRF, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9520734375\n",
      "F1: 0.8863824097019265\n",
      "Recall: 0.8822627195757364\n",
      "Precision: 0.8905407537743798\n"
     ]
    }
   ],
   "source": [
    "evaluate_segmentation(preds_eroded_dilated, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_process(sxy_param, compat_param, diamond_param=3):\n",
    "    labels = []\n",
    "    preds_standard = []\n",
    "    preds_CRF = []\n",
    "    preds_eroded_dilated = []\n",
    "\n",
    "    diamond = morphology.diamond(diamond_param) \n",
    "\n",
    "    for im in sorted(images):\n",
    "        # Get image, segmentation and ground truth\n",
    "        image = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/images/{im}\")\n",
    "        segm = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/predictions_segformer/{im}\")\n",
    "        segm_binary = segm > 0\n",
    "        segm_binary = segm_binary.astype(float)\n",
    "        segm_rgb = gray2rgb(segm)\n",
    "        gt = imread(f\"/Users/arcivelekoglu/Desktop/EPFL/MA3/Machine learning/ML_AGA/data/validation/groundtruth/{im}\")\n",
    "        gt = np.round(gt/255)\n",
    "\n",
    "        # Get the prediction probabilities\n",
    "\n",
    "        model.eval() \n",
    "        image = Image.open(f\"./data/validation/images/{im}\")\n",
    "        # Prepare the image for the model\n",
    "        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=gt.shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        p = torch.nn.functional.softmax(upsampled_logits, dim=1)\n",
    "\n",
    "        p = p[0].cpu().numpy()\n",
    "\n",
    "        # Use the CRF to get the final prediction\n",
    "        # Get the unary from the softmax output\n",
    "        W, H, NLABELS = gt.shape[0], gt.shape[1], 2\n",
    "        U = unary_from_softmax(p)\n",
    "\n",
    "        # Horizontal\n",
    "        d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "        d.setUnaryEnergy(U)\n",
    "        #d.addPairwiseGaussian(sxy=(1, 25), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        d.addPairwiseGaussian(sxy=(sxy_param, 1), compat=compat_param, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        Q, tmp1, tmp2 = d.startInference()\n",
    "        for _ in range(50):\n",
    "            d.stepInference(Q, tmp1, tmp2)\n",
    "        kl1 = d.klDivergence(Q) / (H*W)\n",
    "        map_soln_h = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "        # Vertical\n",
    "        d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "        d.setUnaryEnergy(U)\n",
    "        d.addPairwiseGaussian(sxy=(1, sxy_param), compat=compat_param, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        #d.addPairwiseGaussian(sxy=(25, 1), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "        Q, tmp1, tmp2 = d.startInference()\n",
    "        for _ in range(50):\n",
    "            d.stepInference(Q, tmp1, tmp2)\n",
    "        kl2 = d.klDivergence(Q) / (H*W)\n",
    "        map_soln_v = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "        # Together\n",
    "        map_soln = (map_soln_h + map_soln_v) > 0\n",
    "        soln_array = np.array(map_soln)\n",
    "        soln_array = soln_array.astype(float)\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(1,3,1); plt.imshow(segm_binary);\n",
    "        plt.title('Original'); plt.axis('off');\n",
    "        plt.subplot(1,3,2); plt.imshow(map_soln);\n",
    "        plt.title('MAP Solution with DenseCRF'); plt.axis('off');\n",
    "        plt.subplot(1,3,3); plt.imshow(gt);\n",
    "        plt.title('Ground Truth'); plt.axis('off');\n",
    "        plt.show()\n",
    "\n",
    "        eroded = morphology.binary_erosion(soln_array, diamond)\n",
    "        eroded_dilated = morphology.binary_dilation(eroded, diamond)\n",
    "\n",
    "        # Get the metrics\n",
    "        labels.extend(gt.flatten())\n",
    "        preds_standard.extend(segm_binary.flatten())\n",
    "        preds_CRF.extend(soln_array.flatten())\n",
    "        preds_eroded_dilated.extend(eroded_dilated.flatten())\n",
    "    \n",
    "    f1_a = f1_score(labels, preds_CRF, pos_label=1)\n",
    "    f1_b = f1_score(labels, preds_eroded_dilated, pos_label=1)\n",
    "\n",
    "    return f1_a, f1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sxy=5, compat=5\n",
      "Evaluating sxy=5, compat=10\n",
      "Evaluating sxy=5, compat=15\n",
      "Evaluating sxy=5, compat=20\n",
      "Evaluating sxy=5, compat=25\n",
      "Evaluating sxy=5, compat=30\n",
      "Evaluating sxy=20, compat=5\n",
      "Evaluating sxy=20, compat=10\n",
      "Evaluating sxy=20, compat=15\n",
      "Evaluating sxy=20, compat=20\n",
      "Evaluating sxy=20, compat=25\n",
      "Evaluating sxy=20, compat=30\n",
      "Evaluating sxy=40, compat=5\n",
      "Evaluating sxy=40, compat=10\n",
      "Evaluating sxy=40, compat=15\n",
      "Evaluating sxy=40, compat=20\n",
      "Evaluating sxy=40, compat=25\n",
      "Evaluating sxy=40, compat=30\n",
      "Evaluating sxy=60, compat=5\n",
      "Evaluating sxy=60, compat=10\n",
      "Evaluating sxy=60, compat=15\n",
      "Evaluating sxy=60, compat=20\n",
      "Evaluating sxy=60, compat=25\n",
      "Evaluating sxy=60, compat=30\n",
      "Evaluating sxy=80, compat=5\n",
      "Evaluating sxy=80, compat=10\n",
      "Evaluating sxy=80, compat=15\n",
      "Evaluating sxy=80, compat=20\n",
      "Evaluating sxy=80, compat=25\n",
      "Evaluating sxy=80, compat=30\n",
      "Evaluating sxy=100, compat=5\n",
      "Evaluating sxy=100, compat=10\n",
      "Evaluating sxy=100, compat=15\n",
      "Evaluating sxy=100, compat=20\n",
      "Evaluating sxy=100, compat=25\n",
      "Evaluating sxy=100, compat=30\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters search\n",
    "sxy_params = [5, 20, 40, 60, 80, 100]\n",
    "compat_params = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "results_a = np.zeros((len(sxy_params), len(compat_params)))\n",
    "results_b = np.zeros((len(sxy_params), len(compat_params)))\n",
    "\n",
    "for i, sxy in enumerate(sxy_params):\n",
    "    for j, compat in enumerate(compat_params):\n",
    "        print(f\"Evaluating sxy={sxy}, compat={compat}\")\n",
    "        f1_a, f1_b = complete_process(sxy, compat)\n",
    "        results_a[i,j] = f1_a\n",
    "        results_b[i,j] = f1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for CRF: 0.8882229306708231\n",
      "Best F1 score for Eroded-Dilated: 0.8882004410632572\n",
      "Best parameters for CRF: sxy=40, compat=10\n",
      "Best parameters for Eroded-Dilated: sxy=40, compat=10\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "\n",
    "best_a = np.max(results_a)\n",
    "best_b = np.max(results_b)\n",
    "\n",
    "best_params_a = np.unravel_index(np.argmax(results_a, axis=None), results_a.shape)\n",
    "best_params_b = np.unravel_index(np.argmax(results_b, axis=None), results_b.shape)\n",
    "\n",
    "print(f\"Best F1 score for CRF: {best_a}\")\n",
    "print(f\"Best F1 score for Eroded-Dilated: {best_b}\")\n",
    "print(f\"Best parameters for CRF: sxy={sxy_params[best_params_a[0]]}, compat={compat_params[best_params_a[1]]}\")\n",
    "print(f\"Best parameters for Eroded-Dilated: sxy={sxy_params[best_params_b[0]]}, compat={compat_params[best_params_b[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sxy=25, compat=5\n",
      "Evaluating sxy=25, compat=10\n",
      "Evaluating sxy=25, compat=15\n",
      "Evaluating sxy=25, compat=20\n",
      "Evaluating sxy=30, compat=5\n",
      "Evaluating sxy=30, compat=10\n",
      "Evaluating sxy=30, compat=15\n",
      "Evaluating sxy=30, compat=20\n",
      "Evaluating sxy=32.5, compat=5\n",
      "Evaluating sxy=32.5, compat=10\n",
      "Evaluating sxy=32.5, compat=15\n",
      "Evaluating sxy=32.5, compat=20\n",
      "Evaluating sxy=35, compat=5\n",
      "Evaluating sxy=35, compat=10\n",
      "Evaluating sxy=35, compat=15\n",
      "Evaluating sxy=35, compat=20\n",
      "Evaluating sxy=37.5, compat=5\n",
      "Evaluating sxy=37.5, compat=10\n",
      "Evaluating sxy=37.5, compat=15\n",
      "Evaluating sxy=37.5, compat=20\n",
      "Evaluating sxy=40, compat=5\n",
      "Evaluating sxy=40, compat=10\n",
      "Evaluating sxy=40, compat=15\n",
      "Evaluating sxy=40, compat=20\n",
      "Evaluating sxy=42.5, compat=5\n",
      "Evaluating sxy=42.5, compat=10\n",
      "Evaluating sxy=42.5, compat=15\n",
      "Evaluating sxy=42.5, compat=20\n",
      "Evaluating sxy=45, compat=5\n",
      "Evaluating sxy=45, compat=10\n",
      "Evaluating sxy=45, compat=15\n",
      "Evaluating sxy=45, compat=20\n",
      "Evaluating sxy=47.5, compat=5\n",
      "Evaluating sxy=47.5, compat=10\n",
      "Evaluating sxy=47.5, compat=15\n",
      "Evaluating sxy=47.5, compat=20\n",
      "Evaluating sxy=50, compat=5\n",
      "Evaluating sxy=50, compat=10\n",
      "Evaluating sxy=50, compat=15\n",
      "Evaluating sxy=50, compat=20\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters search\n",
    "sxy_params = [25, 30, 32.5, 35, 37.5, 40, 42.5, 45, 47.5, 50]\n",
    "compat_params = [5, 10, 15, 20]\n",
    "\n",
    "results_a = np.zeros((len(sxy_params), len(compat_params)))\n",
    "results_b = np.zeros((len(sxy_params), len(compat_params)))\n",
    "\n",
    "for i, sxy in enumerate(sxy_params):\n",
    "    for j, compat in enumerate(compat_params):\n",
    "        print(f\"Evaluating sxy={sxy}, compat={compat}\")\n",
    "        f1_a, f1_b = complete_process(sxy, compat)\n",
    "        results_a[i,j] = f1_a\n",
    "        results_b[i,j] = f1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for CRF: 0.8883123222152859\n",
      "Best F1 score for Eroded-Dilated: 0.8882004410632572\n",
      "Best parameters for CRF: sxy=35, compat=15\n",
      "Best parameters for Eroded-Dilated: sxy=40, compat=10\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "\n",
    "best_a = np.max(results_a)\n",
    "best_b = np.max(results_b)\n",
    "\n",
    "best_params_a = np.unravel_index(np.argmax(results_a, axis=None), results_a.shape)\n",
    "best_params_b = np.unravel_index(np.argmax(results_b, axis=None), results_b.shape)\n",
    "\n",
    "print(f\"Best F1 score for CRF: {best_a}\")\n",
    "print(f\"Best F1 score for Eroded-Dilated: {best_b}\")\n",
    "print(f\"Best parameters for CRF: sxy={sxy_params[best_params_a[0]]}, compat={compat_params[best_params_a[1]]}\")\n",
    "print(f\"Best parameters for Eroded-Dilated: sxy={sxy_params[best_params_b[0]]}, compat={compat_params[best_params_b[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating diamond=1\n",
      "Evaluating diamond=2\n",
      "Evaluating diamond=3\n",
      "Evaluating diamond=4\n",
      "Evaluating diamond=5\n",
      "Evaluating diamond=6\n",
      "Evaluating diamond=7\n",
      "Evaluating diamond=8\n",
      "Evaluating diamond=9\n",
      "Evaluating diamond=10\n"
     ]
    }
   ],
   "source": [
    "diamond_params = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "results_a = np.zeros(len(diamond_params))\n",
    "results_b = np.zeros(len(diamond_params))\n",
    "\n",
    "for i, diamond in enumerate(diamond_params):\n",
    "    print(f\"Evaluating diamond={diamond}\")\n",
    "    f1_a, f1_b = complete_process(35, 15, diamond)\n",
    "    results_a[i] = f1_a\n",
    "    results_b[i] = f1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for CRF: 0.8883123222152859\n",
      "Best F1 score for Eroded-Dilated: 0.8883071812943474\n",
      "Best parameters for CRF: diamond=1\n",
      "Best parameters for Eroded-Dilated: diamond=1\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "\n",
    "best_a = np.max(results_a)\n",
    "best_b = np.max(results_b)\n",
    "\n",
    "best_params_a = np.argmax(results_a)\n",
    "best_params_b = np.argmax(results_b)\n",
    "\n",
    "print(f\"Best F1 score for CRF: {best_a}\")\n",
    "print(f\"Best F1 score for Eroded-Dilated: {best_b}\")\n",
    "print(f\"Best parameters for CRF: diamond={diamond_params[best_params_a]}\")\n",
    "print(f\"Best parameters for Eroded-Dilated: diamond={diamond_params[best_params_b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88830718, 0.8882613 , 0.88803458, 0.88742023, 0.88562436,\n",
       "       0.86689281, 0.84627606, 0.84547661, 0.8460542 , 0.84731774])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating diamond=1\n",
      "Evaluating diamond=2\n",
      "Evaluating diamond=3\n",
      "Evaluating diamond=4\n",
      "Evaluating diamond=5\n"
     ]
    }
   ],
   "source": [
    "diamond_params = [1, 2, 3, 4, 5]\n",
    "\n",
    "results_a = np.zeros(len(diamond_params))\n",
    "results_b = np.zeros(len(diamond_params))\n",
    "\n",
    "for i, diamond in enumerate(diamond_params):\n",
    "    print(f\"Evaluating diamond={diamond}\")\n",
    "    f1_a, f1_b = complete_process(40, 10, diamond)\n",
    "    results_a[i] = f1_a\n",
    "    results_b[i] = f1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for CRF: 0.8882229306708231\n",
      "Best F1 score for Eroded-Dilated: 0.888267624310379\n",
      "Best parameters for CRF: diamond=1\n",
      "Best parameters for Eroded-Dilated: diamond=2\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "\n",
    "best_a = np.max(results_a)\n",
    "best_b = np.max(results_b)\n",
    "\n",
    "best_params_a = np.argmax(results_a)\n",
    "best_params_b = np.argmax(results_b)\n",
    "\n",
    "print(f\"Best F1 score for CRF: {best_a}\")\n",
    "print(f\"Best F1 score for Eroded-Dilated: {best_b}\")\n",
    "print(f\"Best parameters for CRF: diamond={diamond_params[best_params_a]}\")\n",
    "print(f\"Best parameters for Eroded-Dilated: diamond={diamond_params[best_params_b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_1\n",
      "test_10\n",
      "test_11\n",
      "test_12\n",
      "test_13\n",
      "test_14\n",
      "test_15\n",
      "test_16\n",
      "test_17\n",
      "test_18\n",
      "test_19\n",
      "test_2\n",
      "test_20\n",
      "test_21\n",
      "test_22\n",
      "test_23\n",
      "test_24\n",
      "test_25\n",
      "test_26\n",
      "test_27\n",
      "test_28\n",
      "test_29\n",
      "test_3\n",
      "test_30\n",
      "test_31\n",
      "test_32\n",
      "test_33\n",
      "test_34\n",
      "test_35\n",
      "test_36\n",
      "test_37\n",
      "test_38\n",
      "test_39\n",
      "test_4\n",
      "test_40\n",
      "test_41\n",
      "test_42\n",
      "test_43\n",
      "test_44\n",
      "test_45\n",
      "test_46\n",
      "test_47\n",
      "test_48\n",
      "test_49\n",
      "test_5\n",
      "test_50\n",
      "test_6\n",
      "test_7\n",
      "test_8\n",
      "test_9\n",
      "Test images processed. Creating submission file...\n",
      "Submission file created. Done!\n"
     ]
    }
   ],
   "source": [
    "# Post process test set\n",
    "\n",
    "images_test = sorted(os.listdir(\"./data/test\"))\n",
    "images_test = [im for im in images_test if im.startswith(\"test\")]\n",
    "images_test = [im for im in images_test if os.path.isdir(f\"./data/test/{im}\")]\n",
    "\n",
    "for im in images_test:\n",
    "    print(im)\n",
    "    # Open the image and the segmentation map\n",
    "    image = imread(f\"./data/test/{im}/{im}.png\")\n",
    "    segm = imread(f\"./data/test/{im}/{im}_pred_segformer_ft_geocropdeg_15.png\")\n",
    "    segm_binary = segm > 0\n",
    "    segm_binary = segm_binary.astype(float)\n",
    "    segm_rgb = gray2rgb(segm)\n",
    "\n",
    "    # Get the prediction probabilities\n",
    "    model.eval() \n",
    "    image = Image.open(f\"./data/test/{im}/{im}.png\")\n",
    "    # Prepare the image for the model\n",
    "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    upsampled_logits = nn.functional.interpolate(logits, size=segm.shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    p = torch.nn.functional.softmax(upsampled_logits, dim=1)\n",
    "    p = p[0].cpu().numpy()\n",
    "\n",
    "    # Use the CRF to get the final prediction\n",
    "    # Get the unary from the softmax output\n",
    "    W, H, NLABELS = segm.shape[0], segm.shape[1], 2\n",
    "    U = unary_from_softmax(p)\n",
    "\n",
    "    # Horizontal\n",
    "    d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "    d.setUnaryEnergy(U)\n",
    "    #d.addPairwiseGaussian(sxy=(1, 25), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    d.addPairwiseGaussian(sxy=(35, 1), compat=15, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    Q, tmp1, tmp2 = d.startInference()\n",
    "    for _ in range(50):\n",
    "        d.stepInference(Q, tmp1, tmp2)\n",
    "    kl1 = d.klDivergence(Q) / (H*W)\n",
    "    map_soln_h = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "    # Vertical\n",
    "    d = dcrf.DenseCRF2D(W, H, NLABELS)\n",
    "    d.setUnaryEnergy(U)\n",
    "    d.addPairwiseGaussian(sxy=(1, 35), compat=15, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseGaussian(sxy=(25, 1), compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    #d.addPairwiseBilateral(sxy=(50, 50), srgb=(5, 5, 5), rgbim=segm_rgb, compat=25, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "    Q, tmp1, tmp2 = d.startInference()\n",
    "    for _ in range(50):\n",
    "        d.stepInference(Q, tmp1, tmp2)\n",
    "    kl2 = d.klDivergence(Q) / (H*W)\n",
    "    map_soln_v = np.argmax(Q, axis=0).reshape((H,W))\n",
    "\n",
    "    # Together\n",
    "    map_soln = (map_soln_h + map_soln_v) > 0\n",
    "    # Can also add original segmentation to regain the diagonal roads lost in the CRF\n",
    "    # map_soln = (map_soln_h + map_soln_v + segm_binary) > 0\n",
    "    soln_array = np.array(map_soln)\n",
    "    soln_array = soln_array.astype(float)\n",
    "\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # plt.subplot(1,3,1); plt.imshow(segm_binary);\n",
    "    # plt.title('Original'); plt.axis('off');\n",
    "    # plt.subplot(1,3,2); plt.imshow(map_soln);\n",
    "    # plt.title('MAP Solution with DenseCRF'); plt.axis('off');\n",
    "    # plt.subplot(1,3,3); plt.imshow(gt);\n",
    "    # plt.title('Ground Truth'); plt.axis('off');\n",
    "    # plt.show()\n",
    "\n",
    "    # Added in case of posprocessing2 - adding back of segm_binary\n",
    "    # eroded = morphology.binary_erosion(soln_array, diamond)\n",
    "    # soln_array = morphology.binary_dilation(eroded, diamond)\n",
    "\n",
    "    soln_array = soln_array*255\n",
    "    soln_array = soln_array.astype(np.uint8)\n",
    "\n",
    "    # Save the segmentation map\n",
    "    segm_im = Image.fromarray(soln_array)\n",
    "    segm_im.save(f\"./data/test/{im}/{im}_postprocessed_pred_segformer_ft_geocropdeg_15.png\")\n",
    "\n",
    "print(\"Test images processed. Creating submission file...\")\n",
    "\n",
    "# Create submission file\n",
    "submission_filename = 'submission_postprocessed_segformer_ft_geocropdeg_15.csv'\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = f\"./data/test/test_{i}/test_{i}_postprocessed_pred_segformer_ft_geocropdeg_15.png\"\n",
    "    # print(image_filename)\n",
    "    image_filenames.append(image_filename)\n",
    "masks_to_submission(submission_filename, *image_filenames)\n",
    "\n",
    "print(\"Submission file created. Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
